{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les packages suivants doivent être installés dans le cas d'utilisation d'AWS\n",
    "#sc.install_pypi_package(\"pip==20.2.4\")\n",
    "#sc.install_pypi_package(\"opencv-python\")\n",
    "#sc.install_pypi_package(\"resize-image\")\n",
    "#sc.install_pypi_package(\"boto3\")\n",
    "#sc.install_pypi_package(\"pandas\")\n",
    "#sc.install_pypi_package(\"scikit-learn\")\n",
    "#sc.install_pypi_package(\"Pillow==7.0.0\")\n",
    "#sc.install_pypi_package(\"pyquickhelper\")\n",
    "#sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================== \n",
      "> > > > Import des librairies < < < < \n",
      "=====================================\n",
      "\n",
      "\n",
      "======================================== \n",
      "> > > > Définition des fonctions < < < < \n",
      "========================================\n",
      "\n",
      "\n",
      "========================================================== \n",
      "> > > > TRAITEMENT DU JEU DE DONNES D'ENTRAINEMENT < < < < \n",
      "==========================================================\n",
      "\n",
      "\n",
      "=========================================================== \n",
      "Identification des chemins d'accès aux répertoires d'images \n",
      "===========================================================\n",
      "\n",
      "Nombre d'images par catégorie (sous-répertoire):\n",
      "\n",
      "    Catégorie  Nombre d'images\n",
      "0       Corn               50\n",
      "1  Raspberry               50\n",
      "2     Orange               50 \n",
      "\n",
      "Nombre total d'images: 150 \n",
      "\n",
      "dataset_path = data/fruits_360_v3b/Training/\n",
      "\n",
      "image_path = data/fruits_360_v3b/Training/Corn,data/fruits_360_v3b/Training/Raspberry,data/fruits_360_v3b/Training/Orange\n",
      "\n",
      "Nombre de catégories de fruits: 3\n",
      "\n",
      "2 premières catégories: ['data/fruits_360_v3b/Training/Corn', 'data/fruits_360_v3b/Training/Raspberry']\n",
      "2 dernières catégories: ['data/fruits_360_v3b/Training/Raspberry', 'data/fruits_360_v3b/Training/Orange']\n",
      "\n",
      "Durée de l'opération 'Récupération des images': 0.01 s\n",
      "\n",
      "\n",
      "======================= \n",
      "Calcul des descripteurs \n",
      "=======================\n",
      "\n",
      "\n",
      "Chargement des images (rdd_images) \n",
      "==================================\n",
      "\n",
      "MapPartitionsRDD[4] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "\n",
      "Nombre de partitions: 5\n",
      "Dimension: 150\n",
      "\n",
      "Catégories / Images / Descripteurs (rdd_cat_ima_desc) \n",
      "=====================================================\n",
      "\n",
      "PythonRDD[9] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Catégories / Images / Descripteurs (rdd_cat_ima_desc_f) \n",
      "=======================================================\n",
      "\n",
      "PythonRDD[10] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Catégories (rdd_cat) \n",
      "====================\n",
      "\n",
      "PythonRDD[14] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Identifiants des images (rdd_ima) \n",
      "=================================\n",
      "\n",
      "PythonRDD[15] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Descripteurs (rdd_desc) \n",
      "=======================\n",
      "\n",
      "PythonRDD[16] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Nombre de partitions: 5\n",
      "Dimension: 11133\n",
      "\n",
      "Collecte des catégories d'images (list_cat) \n",
      "===========================================\n",
      "\n",
      "3 premières occurences: ['Raspberry', 'Raspberry', 'Raspberry']\n",
      "\n",
      "Collecte des identifiants des images (list_ima) \n",
      "===============================================\n",
      "\n",
      "3 premières occurences: ['Raspberry_19_100.jpg', 'Raspberry_19_100.jpg', 'Raspberry_19_100.jpg']\n",
      "df_ima_cat: (11133, 2)\n",
      "df_ima_cat (sans dup): (150, 2)\n",
      "\n",
      "Identifiants des images et des catégories (sdf_ima_cat) \n",
      "=======================================================\n",
      "\n",
      "root\n",
      " |-- ima: string (nullable = true)\n",
      " |-- cat: string (nullable = true)\n",
      "\n",
      "+--------------------+---------+\n",
      "|                 ima|      cat|\n",
      "+--------------------+---------+\n",
      "|Raspberry_19_100.jpg|Raspberry|\n",
      "|Raspberry_14_100.jpg|Raspberry|\n",
      "|Raspberry_43_100.jpg|Raspberry|\n",
      "+--------------------+---------+\n",
      "\n",
      "Durée de l'opération 'Extraction des descripteurs des images': 18.66 s\n",
      "\n",
      "\n",
      "=========================================================== \n",
      "Classification non supervisée des descripteurs avec K-Means \n",
      "===========================================================\n",
      "\n",
      "\n",
      "Modèle K-Means (km_model) \n",
      "=========================\n",
      "\n",
      "<pyspark.mllib.clustering.KMeansModel object at 0x7f1d88097790>\n",
      "\n",
      "Nombre de clusters: 30 \n",
      "\n",
      "Durée de l'opération 'Clustering K-Means': 32.76 s\n",
      "\n",
      "\n",
      "========================================= \n",
      "Prédictions des descripteurs avec K-Means \n",
      "=========================================\n",
      "\n",
      "\n",
      "Prédictions (rdd_km_pred) \n",
      "=========================\n",
      "\n",
      "PythonRDD[226] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Nombre de partitions: 5\n",
      "Dimension: 11133\n",
      "\n",
      "Collecte des prédictions (list_km_pred) \n",
      "=======================================\n",
      "\n",
      "[3, 21, 11, 27, 7, 11, 13, 11, 20, 1] \n",
      "\n",
      "Durée de l'opération 'Prédiction K-Means': 6.2 s\n",
      "\n",
      "\n",
      "======================== \n",
      "Création du bag of words \n",
      "========================\n",
      "\n",
      "\n",
      "Encodage des identifiants d'images et concatenation avec les prédictions (clusters K-Means) \n",
      "===========================================================================================\n",
      "\n",
      "Encodage des identifiants d'images (sdf_ima_label) \n",
      "--------------------------------------------------\n",
      "\n",
      "+--------------------+--------+----------+\n",
      "|                 IMA|image_id|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|Raspberry_19_100.jpg|     110|         3|\n",
      "|Raspberry_19_100.jpg|     110|        21|\n",
      "|Raspberry_19_100.jpg|     110|        11|\n",
      "|Raspberry_19_100.jpg|     110|        27|\n",
      "|Raspberry_19_100.jpg|     110|         7|\n",
      "|Raspberry_19_100.jpg|     110|        11|\n",
      "|Raspberry_19_100.jpg|     110|        13|\n",
      "|Raspberry_19_100.jpg|     110|        11|\n",
      "|Raspberry_19_100.jpg|     110|        20|\n",
      "|Raspberry_19_100.jpg|     110|         1|\n",
      "+--------------------+--------+----------+\n",
      "\n",
      "Prédictions (clusters K-Means) par image (sdf_ima_pred) \n",
      "-------------------------------------------------------\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- prediction: long (nullable = true)\n",
      "\n",
      "+---+----------+\n",
      "| id|prediction|\n",
      "+---+----------+\n",
      "|110|         3|\n",
      "|110|        21|\n",
      "|110|        11|\n",
      "|110|        27|\n",
      "|110|         7|\n",
      "|110|        11|\n",
      "|110|        13|\n",
      "|110|        11|\n",
      "|110|        20|\n",
      "|110|         1|\n",
      "+---+----------+\n",
      "\n",
      "\n",
      "Liste des clusters par image (Map + reduceByKey) \n",
      "================================================\n",
      "\n",
      "Clusters par image (rdd_words) \n",
      "------------------------------\n",
      "\n",
      "PythonRDD[256] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Nombre de partitions: 2\n",
      "Dimension: 150\n",
      "\n",
      "Liste de 'words' par image (sdf_worcds) \n",
      "----------------------------------------\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|image_id|               words|\n",
      "+--------+--------------------+\n",
      "|     110|[3, 21, 11, 27, 7...|\n",
      "|     112|[3, 3, 3, 27, 11,...|\n",
      "|     108|[19, 13, 27, 3, 7...|\n",
      "|     106|[3, 1, 3, 27, 27,...|\n",
      "|     114|[19, 3, 7, 3, 11,...|\n",
      "|     116|[21, 3, 7, 23, 21...|\n",
      "|     140|[27, 3, 8, 15, 27...|\n",
      "|     124|[13, 7, 0, 3, 3, ...|\n",
      "|     102|[13, 3, 21, 27, 3...|\n",
      "|     136|[3, 19, 27, 8, 27...|\n",
      "+--------+--------------------+\n",
      "\n",
      "\n",
      "Création du bag of words à partir des listes de 'words' associées aux images (CountVectorizer) \n",
      "==============================================================================================\n",
      "\n",
      "Bag of words (sdf_bow) \n",
      "----------------------\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- bag_of_words: vector (nullable = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|image_id|        bag_of_words|\n",
      "+--------+--------------------+\n",
      "|     110|(30,[0,1,2,3,4,5,...|\n",
      "|     112|(30,[0,1,2,3,4,5,...|\n",
      "|     108|(30,[0,1,2,3,4,5,...|\n",
      "|     106|(30,[0,1,2,3,4,5,...|\n",
      "|     114|(30,[0,1,2,3,4,5,...|\n",
      "|     116|(30,[0,1,2,3,4,5,...|\n",
      "|     140|(30,[0,1,2,3,4,5,...|\n",
      "|     124|(30,[0,1,2,3,4,5,...|\n",
      "|     102|(30,[0,1,2,3,4,5,...|\n",
      "|     136|(30,[0,1,2,3,4,5,...|\n",
      "+--------+--------------------+\n",
      "\n",
      "\n",
      "========================== \n",
      "Sauvegarde du bag of words \n",
      "==========================\n",
      "\n",
      "\n",
      "Bag of words (df_bow) \n",
      "=====================\n",
      "\n",
      "   image_id                                       bag_of_words\n",
      "0       110  (3.0, 9.0, 4.0, 15.0, 4.0, 9.0, 10.0, 1.0, 3.0...\n",
      "1       112  (2.0, 8.0, 6.0, 13.0, 2.0, 7.0, 13.0, 0.0, 7.0...\n",
      "2       108  (3.0, 12.0, 2.0, 13.0, 4.0, 8.0, 11.0, 3.0, 4....\n",
      "3       106  (2.0, 10.0, 6.0, 14.0, 3.0, 8.0, 15.0, 1.0, 4....\n",
      "4       114  (2.0, 8.0, 4.0, 10.0, 4.0, 7.0, 11.0, 1.0, 5.0... \n",
      "\n",
      "\n",
      "Bag of words \n",
      "============\n",
      "\n",
      "                    ima        cat    0    1     2     3    4     5\n",
      "0  Raspberry_19_100.jpg  Raspberry  3.0  9.0   4.0  15.0  4.0   9.0\n",
      "1  Raspberry_14_100.jpg  Raspberry  2.0  8.0   4.0  12.0  3.0  10.0\n",
      "2  Raspberry_43_100.jpg  Raspberry  2.0  8.0  24.0  11.0  2.0   9.0\n",
      "3  Raspberry_45_100.jpg  Raspberry  2.0  7.0  18.0   9.0  2.0   9.0\n",
      "4  Raspberry_20_100.jpg  Raspberry  2.0  8.0   6.0  13.0  2.0   7.0 \n",
      "\n",
      "Dimensions du jeu de données: (150, 32) \n",
      "\n",
      "Durée de l'opération 'Création du bag of words': 8.28 s\n",
      "\n",
      "\n",
      "========================== \n",
      "Réduction de dimension PCA \n",
      "==========================\n",
      "\n",
      "\n",
      "Résultats de la PCA (sdf_features) \n",
      "==================================\n",
      "\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[28.6589691872513...|\n",
      "|[30.6704889986520...|\n",
      "|[26.6684862365683...|\n",
      "|[31.6841436746050...|\n",
      "|[29.2510349188184...|\n",
      "|[33.9143218745699...|\n",
      "|[35.3151410157159...|\n",
      "|[27.0750731114220...|\n",
      "|[27.7020165980217...|\n",
      "|[31.7533006032619...|\n",
      "+--------------------+\n",
      "\n",
      "\n",
      "Jointure entre les ids des images et les features (sdf_ima_features) \n",
      "====================================================================\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- bag_of_words: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------+--------------------+--------------------+\n",
      "|image_id|        bag_of_words|            features|\n",
      "+--------+--------------------+--------------------+\n",
      "|      76|(30,[0,1,4,7,11,1...|[-2.1029243806689...|\n",
      "|      60|(30,[0,1,4,7,8,14...|[-1.3391962404690...|\n",
      "|      48|(30,[0,1,2,4,8,9,...|[0.77967717300582...|\n",
      "|     144|(30,[0,1,2,3,4,5,...|[29.1377046696358...|\n",
      "|      40|(30,[0,1,2,4,5,8,...|[1.31246907909725...|\n",
      "|     110|(30,[0,1,2,3,4,5,...|[28.6589691872513...|\n",
      "|      97|(30,[0,4,6,7,8,11...|[-0.7051385240676...|\n",
      "|      67|(30,[0,4,7,8,11,1...|[-0.7264771357198...|\n",
      "|     146|(30,[0,1,2,3,4,5,...|[29.8051167935406...|\n",
      "|      19|(30,[0,1,2,3,4,5,...|[3.96508571710604...|\n",
      "+--------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Jointure entre les catégories et les features (sdf_cat_features) \n",
      "================================================================\n",
      "\n",
      "root\n",
      " |-- IMA: string (nullable = true)\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- cat: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+--------------------+---------+\n",
      "|                 IMA|image_id|            features|      cat|\n",
      "+--------------------+--------+--------------------+---------+\n",
      "|    Orange_2_100.jpg|      99|[-1.9381004478848...|   Orange|\n",
      "|  Orange_102_100.jpg|      53|[-0.5822993253601...|   Orange|\n",
      "|  Orange_124_100.jpg|      77|[-1.3910691745482...|   Orange|\n",
      "|   Orange_24_100.jpg|      93|[-1.3747704461391...|   Orange|\n",
      "|Raspberry_17_100.jpg|     108|[26.6684862365683...|Raspberry|\n",
      "|     Corn_11_100.jpg|       2|[2.23326783378635...|     Corn|\n",
      "|     Corn_15_100.jpg|       6|[1.16132418870859...|     Corn|\n",
      "|     Corn_18_100.jpg|       9|[3.17218229130792...|     Corn|\n",
      "|     Corn_44_100.jpg|      23|[2.81897891583134...|     Corn|\n",
      "|  Orange_121_100.jpg|      74|[-0.8462483779417...|   Orange|\n",
      "+--------------------+--------+--------------------+---------+\n",
      "\n",
      "\n",
      "Encodage de la variable catégories (sdf_lab_features) \n",
      "=====================================================\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[-1.9381004478848...|\n",
      "|  1.0|[-0.5822993253601...|\n",
      "|  1.0|[-1.3910691745482...|\n",
      "|  1.0|[-1.3747704461391...|\n",
      "|  2.0|[26.6684862365683...|\n",
      "|  0.0|[2.23326783378635...|\n",
      "|  0.0|[1.16132418870859...|\n",
      "|  0.0|[3.17218229130792...|\n",
      "|  0.0|[2.81897891583134...|\n",
      "|  1.0|[-0.8462483779417...|\n",
      "+-----+--------------------+\n",
      "\n",
      "\n",
      "Bag of words après réduction de dimension (df_lab_features) \n",
      "===========================================================\n",
      "\n",
      "   label          0         1         2         3         4         5\n",
      "0    1.0  -1.938100  4.373330  7.230668 -6.354446  6.046578 -7.506347\n",
      "1    1.0  -0.582299  3.418589  5.567492 -7.003518  3.878954 -5.515348\n",
      "2    1.0  -1.391069  3.003477  5.123474 -6.696992  4.103136 -6.786919\n",
      "3    1.0  -1.374770  3.939350  5.026782 -4.804279  1.135001 -5.934161\n",
      "4    2.0  26.668486  7.078681 -0.799438 -4.180231  5.872323 -3.987865\n",
      "5    0.0   2.233268 -2.347001 -2.344602 -3.261785  3.304597 -6.851918\n",
      "6    0.0   1.161324 -1.819158  0.087453 -6.264102  5.091056 -5.406186\n",
      "7    0.0   3.172182  0.122427  0.194158 -9.281862  8.285937 -5.758286\n",
      "8    0.0   2.818979 -4.847871 -2.529770 -4.401651  4.233620 -6.630153\n",
      "\n",
      "Dimensions du nouveau jeu de données avec les étiquettes (df_lab_features): (150, 22)\n",
      "\n",
      "Durée de l'opération 'Réduction de dimension': 72.5 s\n",
      "\n",
      "\n",
      "================== \n",
      "Classification MLP \n",
      "==================\n",
      "\n",
      "Modèle:\n",
      "\n",
      " MultilayerPerceptronClassificationModel: uid=MultilayerPerceptronClassifier_572046d74e49, numLayers=4, numClasses=3, numFeatures=21 \n",
      "\n",
      "Durée de l'opération 'Classification': 88.62 s\n",
      "\n",
      "\n",
      "==================================================== \n",
      "> > > > TRAITEMENT DU JEU DE DONNEES DE TEST < < < < \n",
      "====================================================\n",
      "\n",
      "\n",
      "=========================================================== \n",
      "Identification des chemins d'accès aux répertoires d'images \n",
      "===========================================================\n",
      "\n",
      "Nombre d'images par catégorie (sous-répertoire):\n",
      "\n",
      "    Catégorie  Nombre d'images\n",
      "0       Corn               25\n",
      "1  Raspberry               25\n",
      "2     Orange               25 \n",
      "\n",
      "Nombre total d'images: 75 \n",
      "\n",
      "dataset_path = data/fruits_360_v3b/Test/\n",
      "\n",
      "image_path = data/fruits_360_v3b/Test/Corn,data/fruits_360_v3b/Test/Raspberry,data/fruits_360_v3b/Test/Orange\n",
      "\n",
      "Nombre de catégories de fruits: 3\n",
      "\n",
      "2 premières catégories: ['data/fruits_360_v3b/Test/Corn', 'data/fruits_360_v3b/Test/Raspberry']\n",
      "2 dernières catégories: ['data/fruits_360_v3b/Test/Raspberry', 'data/fruits_360_v3b/Test/Orange']\n",
      "\n",
      "Durée de l'opération 'Récupération des images - Test': 0.01 s\n",
      "\n",
      "\n",
      "======================= \n",
      "Calcul des descripteurs \n",
      "=======================\n",
      "\n",
      "\n",
      "Chargement des images (rdd_images) \n",
      "==================================\n",
      "\n",
      "MapPartitionsRDD[651] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "\n",
      "Nombre de partitions: 3\n",
      "Dimension: 75\n",
      "\n",
      "Catégories / Images / Descripteurs (rdd_cat_ima_desc) \n",
      "=====================================================\n",
      "\n",
      "PythonRDD[656] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Catégories / Images / Descripteurs (rdd_cat_ima_desc_f) \n",
      "=======================================================\n",
      "\n",
      "PythonRDD[657] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Catégories (rdd_cat) \n",
      "====================\n",
      "\n",
      "PythonRDD[661] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Identifiants des images (rdd_ima) \n",
      "=================================\n",
      "\n",
      "PythonRDD[662] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Descripteurs (rdd_desc) \n",
      "=======================\n",
      "\n",
      "PythonRDD[663] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Nombre de partitions: 3\n",
      "Dimension: 5223\n",
      "\n",
      "Collecte des catégories d'images (list_cat) \n",
      "===========================================\n",
      "\n",
      "3 premières occurences: ['Raspberry', 'Raspberry', 'Raspberry']\n",
      "\n",
      "Collecte des identifiants des images (list_ima) \n",
      "===============================================\n",
      "\n",
      "3 premières occurences: ['Raspberry_80_100.jpg', 'Raspberry_80_100.jpg', 'Raspberry_80_100.jpg']\n",
      "df_ima_cat: (5223, 2)\n",
      "df_ima_cat (sans dup): (75, 2)\n",
      "\n",
      "Identifiants des images et des catégories (sdf_ima_cat) \n",
      "=======================================================\n",
      "\n",
      "root\n",
      " |-- ima: string (nullable = true)\n",
      " |-- cat: string (nullable = true)\n",
      "\n",
      "+--------------------+---------+\n",
      "|                 ima|      cat|\n",
      "+--------------------+---------+\n",
      "|Raspberry_80_100.jpg|Raspberry|\n",
      "|Raspberry_77_100.jpg|Raspberry|\n",
      "|Raspberry_79_100.jpg|Raspberry|\n",
      "+--------------------+---------+\n",
      "\n",
      "Durée de l'opération 'Extraction des descripteurs des images - Test': 3.33 s\n",
      "\n",
      "\n",
      "========================================= \n",
      "Prédictions des descripteurs avec K-Means \n",
      "=========================================\n",
      "\n",
      "\n",
      "Prédictions (rdd_km_pred) \n",
      "=========================\n",
      "\n",
      "PythonRDD[672] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Nombre de partitions: 3\n",
      "Dimension: 5223\n",
      "\n",
      "Collecte des prédictions (list_km_pred) \n",
      "=======================================\n",
      "\n",
      "[3, 3, 3, 25, 7, 19, 11, 0, 27, 1] \n",
      "\n",
      "Durée de l'opération 'Prédiction K-Means - Test': 5.09 s\n",
      "\n",
      "\n",
      "======================== \n",
      "Création du bag of words \n",
      "========================\n",
      "\n",
      "\n",
      "Encodage des identifiants d'images et concatenation avec les prédictions (clusters K-Means) \n",
      "===========================================================================================\n",
      "\n",
      "Encodage des identifiants d'images (sdf_ima_label) \n",
      "--------------------------------------------------\n",
      "\n",
      "+--------------------+--------+----------+\n",
      "|                 IMA|image_id|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|Raspberry_80_100.jpg|      55|         3|\n",
      "|Raspberry_80_100.jpg|      55|         3|\n",
      "|Raspberry_80_100.jpg|      55|         3|\n",
      "|Raspberry_80_100.jpg|      55|        25|\n",
      "|Raspberry_80_100.jpg|      55|         7|\n",
      "|Raspberry_80_100.jpg|      55|        19|\n",
      "|Raspberry_80_100.jpg|      55|        11|\n",
      "|Raspberry_80_100.jpg|      55|         0|\n",
      "|Raspberry_80_100.jpg|      55|        27|\n",
      "|Raspberry_80_100.jpg|      55|         1|\n",
      "+--------------------+--------+----------+\n",
      "\n",
      "Prédictions (clusters K-Means) par image (sdf_ima_pred) \n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- prediction: long (nullable = true)\n",
      "\n",
      "+---+----------+\n",
      "| id|prediction|\n",
      "+---+----------+\n",
      "| 55|         3|\n",
      "| 55|         3|\n",
      "| 55|         3|\n",
      "| 55|        25|\n",
      "| 55|         7|\n",
      "| 55|        19|\n",
      "| 55|        11|\n",
      "| 55|         0|\n",
      "| 55|        27|\n",
      "| 55|         1|\n",
      "+---+----------+\n",
      "\n",
      "\n",
      "Liste des clusters par image (Map + reduceByKey) \n",
      "================================================\n",
      "\n",
      "Clusters par image (rdd_words) \n",
      "------------------------------\n",
      "\n",
      "PythonRDD[702] at RDD at PythonRDD.scala:53\n",
      "\n",
      "Nombre de partitions: 2\n",
      "Dimension: 75\n",
      "\n",
      "Liste de 'words' par image (sdf_worcds) \n",
      "----------------------------------------\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|image_id|               words|\n",
      "+--------+--------------------+\n",
      "|      54|[3, 3, 3, 25, 21,...|\n",
      "|      56|[13, 3, 3, 25, 21...|\n",
      "|      60|[19, 25, 7, 27, 3...|\n",
      "|      66|[19, 25, 25, 7, 2...|\n",
      "|      62|[19, 19, 25, 15, ...|\n",
      "|      58|[19, 3, 13, 3, 19...|\n",
      "|      64|[25, 25, 15, 3, 3...|\n",
      "|      68|[25, 13, 7, 3, 21...|\n",
      "|      52|[19, 3, 3, 3, 19,...|\n",
      "|      70|[7, 3, 27, 27, 11...|\n",
      "+--------+--------------------+\n",
      "\n",
      "\n",
      "Création du bag of words à partir des listes de 'words' associées aux images (CountVectorizer) \n",
      "==============================================================================================\n",
      "\n",
      "Bag of words (sdf_bow) \n",
      "----------------------\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- bag_of_words: vector (nullable = true)\n",
      "\n",
      "+--------+--------------------+\n",
      "|image_id|        bag_of_words|\n",
      "+--------+--------------------+\n",
      "|      54|(30,[0,1,2,3,4,5,...|\n",
      "|      56|(30,[0,1,2,3,4,5,...|\n",
      "|      60|(30,[0,1,2,3,4,5,...|\n",
      "|      66|(30,[0,1,2,3,4,5,...|\n",
      "|      62|(30,[0,1,2,3,4,5,...|\n",
      "|      58|(30,[0,1,2,3,4,5,...|\n",
      "|      64|(30,[0,1,2,3,4,5,...|\n",
      "|      68|(30,[0,1,2,3,4,5,...|\n",
      "|      52|(30,[0,2,3,4,5,6,...|\n",
      "|      70|(30,[0,1,2,3,4,5,...|\n",
      "+--------+--------------------+\n",
      "\n",
      "\n",
      "========================== \n",
      "Sauvegarde du bag of words \n",
      "==========================\n",
      "\n",
      "\n",
      "Bag of words (df_bow) \n",
      "=====================\n",
      "\n",
      "   image_id                                       bag_of_words\n",
      "0        54  (5.0, 5.0, 5.0, 13.0, 9.0, 9.0, 11.0, 9.0, 10....\n",
      "1        56  (3.0, 8.0, 3.0, 6.0, 7.0, 10.0, 9.0, 5.0, 9.0,...\n",
      "2        60  (6.0, 4.0, 3.0, 9.0, 7.0, 9.0, 8.0, 6.0, 6.0, ...\n",
      "3        66  (1.0, 4.0, 5.0, 5.0, 9.0, 6.0, 3.0, 9.0, 10.0,...\n",
      "4        62  (4.0, 3.0, 2.0, 9.0, 11.0, 5.0, 7.0, 10.0, 9.0... \n",
      "\n",
      "\n",
      "Bag of words \n",
      "============\n",
      "\n",
      "                    ima        cat    0    1    2     3    4     5\n",
      "0  Raspberry_80_100.jpg  Raspberry  1.0  8.0  3.0   8.0  9.0  14.0\n",
      "1  Raspberry_77_100.jpg  Raspberry  3.0  3.0  2.0   7.0  4.0  14.0\n",
      "2  Raspberry_79_100.jpg  Raspberry  5.0  5.0  5.0  13.0  9.0   9.0\n",
      "3  Raspberry_81_100.jpg  Raspberry  3.0  8.0  3.0   6.0  7.0  10.0\n",
      "4  Raspberry_82_100.jpg  Raspberry  5.0  7.0  5.0   7.0  9.0  13.0 \n",
      "\n",
      "Dimensions du jeu de données: (75, 32) \n",
      "\n",
      "Durée de l'opération 'Création du bag of words - Test': 4.88 s\n",
      "\n",
      "========================== \n",
      "Réduction de dimension PCA \n",
      "==========================\n",
      "\n",
      "\n",
      "Résultats de la PCA (sdf_features) \n",
      "==================================\n",
      "\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[30.0510914065737...|\n",
      "|[26.1523344783714...|\n",
      "|[23.9139131399411...|\n",
      "|[25.5135494776514...|\n",
      "|[26.4668964038766...|\n",
      "|[30.1364368364823...|\n",
      "|[24.6132670646011...|\n",
      "|[25.5823342822967...|\n",
      "|[24.7349439639706...|\n",
      "|[25.4028440202219...|\n",
      "+--------------------+\n",
      "\n",
      "\n",
      "Jointure entre les ids des images et les features (sdf_ima_features) \n",
      "====================================================================\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- bag_of_words: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------+--------------------+--------------------+\n",
      "|image_id|        bag_of_words|            features|\n",
      "+--------+--------------------+--------------------+\n",
      "|       6|(30,[0,1,3,4,5,6,...|[1.88671564762636...|\n",
      "|      10|(30,[0,1,3,6,7,9,...|[0.39603563099539...|\n",
      "|       5|(30,[0,1,2,3,4,7,...|[2.14251332966443...|\n",
      "|      30|(30,[0,1,2,3,6,9,...|[-0.6024618689648...|\n",
      "|      41|(30,[0,1,2,3,6,8,...|[0.67371931039150...|\n",
      "|      54|(30,[0,1,2,3,4,5,...|[30.0510914065737...|\n",
      "|      26|(30,[0,1,2,6,9,14...|[-1.5597208874028...|\n",
      "|      68|(30,[0,1,2,3,4,5,...|[25.5823342822967...|\n",
      "|      20|(30,[0,1,2,3,5,6,...|[2.50937028459530...|\n",
      "|      35|(30,[0,1,2,3,5,10...|[-0.4043484642786...|\n",
      "+--------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Jointure entre les catégories et les features (sdf_cat_features) \n",
      "================================================================\n",
      "\n",
      "root\n",
      " |-- IMA: string (nullable = true)\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- cat: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+--------------------+---------+\n",
      "|                 IMA|image_id|            features|      cat|\n",
      "+--------------------+--------+--------------------+---------+\n",
      "|Raspberry_87_100.jpg|      62|[26.4668964038766...|Raspberry|\n",
      "|    Orange_3_100.jpg|      35|[-0.4043484642786...|   Orange|\n",
      "|Raspberry_82_100.jpg|      57|[28.6641195896339...|Raspberry|\n",
      "|      Corn_2_100.jpg|       9|[5.18647680516223...|     Corn|\n",
      "|   Orange_43_100.jpg|      39|[-0.0956747951576...|   Orange|\n",
      "|Raspberry_100_100...|      50|[24.8632082783237...|Raspberry|\n",
      "|Raspberry_98_100.jpg|      73|[25.6584718118259...|Raspberry|\n",
      "|     Corn_20_100.jpg|       1|[3.64587116333794...|     Corn|\n",
      "|   Orange_39_100.jpg|      34|[-1.0479874383580...|   Orange|\n",
      "|     Corn_29_100.jpg|       8|[2.32739838521921...|     Corn|\n",
      "+--------------------+--------+--------------------+---------+\n",
      "\n",
      "\n",
      "Encodage de la variable catégories (sdf_lab_features) \n",
      "=====================================================\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  2.0|[26.4668964038766...|\n",
      "|  1.0|[-0.4043484642786...|\n",
      "|  2.0|[28.6641195896339...|\n",
      "|  0.0|[5.18647680516223...|\n",
      "|  1.0|[-0.0956747951576...|\n",
      "|  2.0|[24.8632082783237...|\n",
      "|  2.0|[25.6584718118259...|\n",
      "|  0.0|[3.64587116333794...|\n",
      "|  1.0|[-1.0479874383580...|\n",
      "|  0.0|[2.32739838521921...|\n",
      "+-----+--------------------+\n",
      "\n",
      "\n",
      "Bag of words après réduction de dimension (df_lab_features) \n",
      "===========================================================\n",
      "\n",
      "   label          0         1          2         3         4         5\n",
      "0    2.0  26.466896 -0.037070   4.952361 -8.969748 -4.504904 -2.318564\n",
      "1    1.0  -0.404348  4.709597   7.649410 -2.661897 -1.153152 -2.915067\n",
      "2    2.0  28.664120  0.477897  13.081357 -1.182491 -0.783142 -5.041057\n",
      "3    0.0   5.186477 -2.816440   3.138431 -2.043061 -1.124801 -3.906776\n",
      "4    1.0  -0.095675  6.139206   8.612166 -3.342973 -2.877894 -3.623146\n",
      "5    2.0  24.863208  3.969479   0.115875 -5.347710 -0.081041 -2.178174\n",
      "6    2.0  25.658472  4.911228   0.661215 -3.971885  5.415495 -5.078884\n",
      "7    0.0   3.645871 -3.190890   4.703170 -1.548317 -3.569367 -6.738843\n",
      "8    1.0  -1.047987  4.515285   6.073193 -3.585146  0.104125 -2.443161\n",
      "\n",
      "Dimensions du nouveau jeu de données avec les étiquettes (df_lab_features): (75, 22)\n",
      "\n",
      "Durée de l'opération 'Réduction de dimension - Test': 50.54 s\n",
      "\n",
      "\n",
      "=============== \n",
      "Prédictions MLP \n",
      "===============\n",
      "\n",
      "\n",
      "Prédictions (test_lab_pred) \n",
      "===========================\n",
      "\n",
      "DataFrame[features: vector, prediction: double]\n",
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[30.0510914065737...|       2.0|\n",
      "|[26.1523344783714...|       2.0|\n",
      "|[23.9139131399411...|       2.0|\n",
      "|[25.5135494776514...|       2.0|\n",
      "|[26.4668964038766...|       2.0|\n",
      "|[30.1364368364823...|       2.0|\n",
      "|[24.6132670646011...|       2.0|\n",
      "|[25.5823342822967...|       2.0|\n",
      "|[24.7349439639706...|       2.0|\n",
      "|[25.4028440202219...|       2.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "\n",
      "Prédictions (predictionAndLabels_2) \n",
      "===================================\n",
      "\n",
      "DataFrame[prediction: double, label: double, features: vector]\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       2.0|  2.0|[26.4668964038766...|\n",
      "|       1.0|  1.0|[-0.4043484642786...|\n",
      "|       2.0|  2.0|[28.6641195896339...|\n",
      "|       2.0|  0.0|[5.18647680516223...|\n",
      "|       1.0|  1.0|[-0.0956747951576...|\n",
      "|       2.0|  2.0|[24.8632082783237...|\n",
      "|       2.0|  2.0|[25.6584718118259...|\n",
      "|       2.0|  0.0|[3.64587116333794...|\n",
      "|       1.0|  1.0|[-1.0479874383580...|\n",
      "|       1.0|  0.0|[2.32739838521921...|\n",
      "+----------+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (MLP) = 0.72\n",
      "\n",
      "Durée de l'opération 'Prédiction - Test': 18.71 s\n",
      "\n",
      "\n",
      "========== \n",
      "Evaluation \n",
      "==========\n",
      "\n",
      "\n",
      "Jointure entre les identifiants des images et les features (sdf_ima_features) \n",
      "=============================================================================\n",
      "\n",
      "root\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- bag_of_words: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+--------+--------------------+--------------------+----------+\n",
      "|image_id|        bag_of_words|            features|prediction|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "|       6|(30,[0,1,3,4,5,6,...|[1.88671564762636...|       0.0|\n",
      "|      10|(30,[0,1,3,6,7,9,...|[0.39603563099539...|       1.0|\n",
      "|       5|(30,[0,1,2,3,4,7,...|[2.14251332966443...|       1.0|\n",
      "|      30|(30,[0,1,2,3,6,9,...|[-0.6024618689648...|       1.0|\n",
      "|      41|(30,[0,1,2,3,6,8,...|[0.67371931039150...|       1.0|\n",
      "|      54|(30,[0,1,2,3,4,5,...|[30.0510914065737...|       2.0|\n",
      "|      26|(30,[0,1,2,6,9,14...|[-1.5597208874028...|       1.0|\n",
      "|      68|(30,[0,1,2,3,4,5,...|[25.5823342822967...|       2.0|\n",
      "|      20|(30,[0,1,2,3,5,6,...|[2.50937028459530...|       2.0|\n",
      "|      35|(30,[0,1,2,3,5,10...|[-0.4043484642786...|       1.0|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "\n",
      "\n",
      "Jointure entre les catégories et les features (sdf_cat_features) \n",
      "================================================================\n",
      "\n",
      "root\n",
      " |-- IMA: string (nullable = true)\n",
      " |-- image_id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- cat: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+--------------------+----------+---------+\n",
      "|                 IMA|image_id|            features|prediction|      cat|\n",
      "+--------------------+--------+--------------------+----------+---------+\n",
      "|Raspberry_87_100.jpg|      62|[26.4668964038766...|       2.0|Raspberry|\n",
      "|    Orange_3_100.jpg|      35|[-0.4043484642786...|       1.0|   Orange|\n",
      "|Raspberry_82_100.jpg|      57|[28.6641195896339...|       2.0|Raspberry|\n",
      "|      Corn_2_100.jpg|       9|[5.18647680516223...|       2.0|     Corn|\n",
      "|   Orange_43_100.jpg|      39|[-0.0956747951576...|       1.0|   Orange|\n",
      "|Raspberry_100_100...|      50|[24.8632082783237...|       2.0|Raspberry|\n",
      "|Raspberry_98_100.jpg|      73|[25.6584718118259...|       2.0|Raspberry|\n",
      "|     Corn_20_100.jpg|       1|[3.64587116333794...|       2.0|     Corn|\n",
      "|   Orange_39_100.jpg|      34|[-1.0479874383580...|       1.0|   Orange|\n",
      "|     Corn_29_100.jpg|       8|[2.32739838521921...|       1.0|     Corn|\n",
      "+--------------------+--------+--------------------+----------+---------+\n",
      "\n",
      "\n",
      "Encodage de la variable catégories (sdf_lab_features) \n",
      "=====================================================\n",
      "\n",
      "root\n",
      " |-- ima: string (nullable = true)\n",
      " |-- cat: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+---------+-----+----------+--------------------+\n",
      "|                 ima|      cat|label|prediction|            features|\n",
      "+--------------------+---------+-----+----------+--------------------+\n",
      "|Raspberry_87_100.jpg|Raspberry|  2.0|       2.0|[26.4668964038766...|\n",
      "|    Orange_3_100.jpg|   Orange|  1.0|       1.0|[-0.4043484642786...|\n",
      "|Raspberry_82_100.jpg|Raspberry|  2.0|       2.0|[28.6641195896339...|\n",
      "|      Corn_2_100.jpg|     Corn|  0.0|       2.0|[5.18647680516223...|\n",
      "|   Orange_43_100.jpg|   Orange|  1.0|       1.0|[-0.0956747951576...|\n",
      "|Raspberry_100_100...|Raspberry|  2.0|       2.0|[24.8632082783237...|\n",
      "|Raspberry_98_100.jpg|Raspberry|  2.0|       2.0|[25.6584718118259...|\n",
      "|     Corn_20_100.jpg|     Corn|  0.0|       2.0|[3.64587116333794...|\n",
      "|   Orange_39_100.jpg|   Orange|  1.0|       1.0|[-1.0479874383580...|\n",
      "|     Corn_29_100.jpg|     Corn|  0.0|       1.0|[2.32739838521921...|\n",
      "+--------------------+---------+-----+----------+--------------------+\n",
      "\n",
      "\n",
      "Catégories réelles (label) vs Prédictions (prediction) \n",
      "======================================================\n",
      "\n",
      "prediction   0.0    1.0    2.0\n",
      "label                         \n",
      "0.0         16.0   56.0   28.0\n",
      "1.0          0.0  100.0    0.0\n",
      "2.0          0.0    0.0  100.0\n",
      "\n",
      "Durée de l'opération 'Evaluation - Test': 50.56 s\n",
      "\n",
      "Durée de l'opération 'Fin des traitements': 0.0 s\n",
      "Durée totale de traitement: 00 h 06 m 00 s\n",
      "\n",
      "Durée des opérations\n",
      "--------------------\n",
      "                                        Opération  Durée Estimation\n",
      "0                         Récupération des images   0.01           \n",
      "1          Extraction des descripteurs des images  18.66           \n",
      "2                              Clustering K-Means  32.76           \n",
      "3                              Prédiction K-Means   6.20           \n",
      "4                        Création du bag of words   8.28           \n",
      "5                          Réduction de dimension  72.50           \n",
      "6                                  Classification  88.62           \n",
      "7                  Récupération des images - Test   0.01           \n",
      "8   Extraction des descripteurs des images - Test   3.33           \n",
      "9                       Prédiction K-Means - Test   5.09           \n",
      "10                Création du bag of words - Test   4.88           \n",
      "11                  Réduction de dimension - Test  50.54           \n",
      "12                              Prédiction - Test  18.71           \n",
      "13                              Evaluation - Test  50.56           \n",
      "14                            Fin des traitements   0.00           \n",
      "\n",
      "===================================== \n",
      "> > > > Traitements finalisés < < < < \n",
      "=====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### PARAMETRES ###\n",
    "##################\n",
    "\n",
    "\n",
    "LOCAL = True # Serveur local (True) - Cloud (False)\n",
    "LOC_EXP = True # Export de RDD en fichier texte (uniquement si LOCAL=True)\n",
    "FOLDER = 'data/fruits_360_v3b/' # Chemin du jeu de données (images de fruits)\n",
    "MIN_PARTITION = 18 # Nombre minimum de partitions (initialisation)\n",
    "OPENCV = 'sift' # Méthode de calcul des descripteurs ('sift' ou 'orb')\n",
    "CLS = True # Lancement du processus de classification (True)\n",
    "OPT = False # Fonctions supplémentaires (True)\n",
    "rs_ = 42 # Random state\n",
    "\n",
    "# Format des titres\n",
    "def title(title,level):\n",
    "    if level == 1:\n",
    "        print('\\n'+'='*len(title),'\\n'+title,'\\n'+'='*len(title)+'\\n')\n",
    "    elif level == 2:\n",
    "        print('\\n'+title,'\\n'+'='*len(title)+'\\n')\n",
    "    elif level == 3:\n",
    "        print(title,'\\n'+'-'*len(title)+'\\n')\n",
    "\n",
    "        \n",
    "#############################\n",
    "### IMPORT DES LIBRAIRIES ###\n",
    "#############################\n",
    "\n",
    "\n",
    "title(\"> > > > Import des librairies < < < <\",1)\n",
    "\n",
    "# Librairies système\n",
    "import io\n",
    "from io import StringIO\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "from pyquickhelper.filehelper import remove_folder\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "\n",
    "# Traitement des données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Traitement des images\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import cv2\n",
    "\n",
    "# Librairies Pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Librairies S3\n",
    "import boto.s3\n",
    "import boto3\n",
    "\n",
    "\n",
    "################################\n",
    "### DEFINITION DES FONCTIONS ###\n",
    "################################\n",
    "\n",
    "\n",
    "title(\"> > > > Définition des fonctions < < < <\",1)\n",
    "\n",
    "list_elapsed = []\n",
    "list_elapsed_mem = [] # Mémorisation d'un résultat\n",
    "list_ope = []\n",
    "\n",
    "def time_calc(ope='dernière operation', elapsed_mem='', print_tot=0, print_all=0):\n",
    "    \n",
    "    elapsed = round((time.time() - t),2)\n",
    "    \n",
    "    if ope != 'dernière operation':\n",
    "        print('Durée de l\\'opération %r:' %ope, round(elapsed,2),'s')\n",
    "    else:\n",
    "        print('Durée de l\\'opération:', round(elapsed,2),'s')\n",
    "    \n",
    "    list_elapsed.extend([elapsed])\n",
    "    list_elapsed_mem.extend([elapsed_mem])\n",
    "    list_ope.extend([ope])\n",
    "    tot_duration = sum(list_elapsed)\n",
    "    df_time = pd.DataFrame({'Opération': list_ope, 'Durée': list_elapsed, 'Estimation': list_elapsed_mem})\n",
    "\n",
    "    if print_tot == 1:\n",
    "        print('Durée totale de traitement:',strftime('%H', gmtime(tot_duration)),'h',\n",
    "              strftime('%M', gmtime(tot_duration)),'m',strftime('%S', gmtime(tot_duration)),'s')\n",
    "    \n",
    "    if print_all == 1:\n",
    "        print('')\n",
    "        print('Durée des opérations')\n",
    "        print(len('Durée des opérations')*'-')\n",
    "        print(df_time)  \n",
    "\n",
    "\n",
    "def clean(folder):\n",
    "    if os.path.exists(folder):\n",
    "        return remove_folder(folder)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def part_dim(rdd):\n",
    "    print('\\nNombre de partitions:',rdd.getNumPartitions())\n",
    "    print('Dimension:',rdd.count())\n",
    "\n",
    "\n",
    "def get_path(data_set):\n",
    "    \"\"\" Identification des chemins d'accès aux répertoires d'images.\n",
    "    \n",
    "    - Le paramètre data_set est une chaîne de caractères qui indique le nom du répertoire\n",
    "    contenant les sous-répertoires des images à traiter. Il prend les valeurs '/Training/' ou '/Test/'.    \n",
    "    - La variable globale LOCAL indique la localisation des fichiers (local ou S3).    \n",
    "    - La variable globale FOLDER indique le chemin d'accès au répertoire principal contenant\n",
    "    les répertoires '/Training/' et '/Test/'.    \n",
    "    - La fonction renvoie, sous la forme d'un chaîne de caractère, les chemins d'accès à l'ensemble\n",
    "    des sous-répertoires contenant les images.\n",
    "    \"\"\"\n",
    "    \n",
    "    title(\"Identification des chemins d'accès aux répertoires d'images\",1)\n",
    "    \n",
    "    image_path = ''\n",
    "    \n",
    "    list_dir = []\n",
    "    list_file = []\n",
    "    \n",
    "    if LOCAL:\n",
    "        image_folder = FOLDER + data_set\n",
    "        for root, directories, files in os.walk(image_folder):\n",
    "            list_file.append(len(files))\n",
    "            \n",
    "            for file in directories:\n",
    "                # Pour chaque chemin de sous-répertoire de fruits\n",
    "                # On récupère le dernier élément du chemin\n",
    "                sub_folder = os.path.join(root, file).split('/')[-1]\n",
    "                image_path = image_path + image_folder + sub_folder + ','            \n",
    "                list_dir.append(sub_folder)           \n",
    "    \n",
    "        del list_file[0]\n",
    "        df_dir_files = pd.DataFrame()\n",
    "        df_dir_files['Catégorie'] = list_dir\n",
    "        df_dir_files['Nombre d\\'images'] = list_file\n",
    "        print('Nombre d\\'images par catégorie (sous-répertoire):\\n\\n',df_dir_files.head(),'\\n')\n",
    "        print('Nombre total d\\'images:',sum(list_file),'\\n')\n",
    "                \n",
    "    else:\n",
    "        image_folder = 's3://oc-ds-p8/' + FOLDER + data_set\n",
    "        conn = boto.s3.connect_to_region('eu-west-1')\n",
    "        bucket = conn.get_bucket('oc-ds-p8')\n",
    "        folders = bucket.list(prefix = FOLDER + data_set, delimiter='/')\n",
    "        for folder in folders:\n",
    "            sub_folder = folder.name.split('/')[-2]\n",
    "            image_path = image_path + image_folder + folder.name.split('/')[-2] + '/' + ','\n",
    "\n",
    "    # On supprime la virgule de fin (dernier caractère)\n",
    "    image_path = image_path[0:-1]\n",
    "    # Liste des chemins des sous-répertoires de fruits (catégories)\n",
    "    cat_path = image_path.split(',')\n",
    "    # Nombre de catégories\n",
    "    nb_cat = len(cat_path)\n",
    "    \n",
    "    dataset_path = FOLDER + data_set\n",
    "    print('dataset_path =',dataset_path)\n",
    "    print('\\nimage_path =',image_path)    \n",
    "    \n",
    "    print('\\nNombre de catégories de fruits:',nb_cat)\n",
    "    print('\\n2 premières catégories:',cat_path[0:2])\n",
    "    print('2 dernières catégories:',cat_path[-2:])\n",
    "    print('')\n",
    "    \n",
    "    return nb_cat, image_path, dataset_path\n",
    "\n",
    "\n",
    "def get_descriptors(image):\n",
    "    \"\"\" La fonction prend en entrée le fichier binaire d'une image et procède à son traitement.\n",
    "    \n",
    "    - Transformation du fichier binaire.    \n",
    "    - Traitement.\n",
    "    - Calcul des descripteurs avec openCV sur chaque image.    \n",
    "    - La fonction renvoie le nom de l'image comme clé et la liste des descripteurs comme valeur.\n",
    "    \"\"\" \n",
    "    \n",
    "    try:\n",
    "        # Transformation du fichier binaire en image\n",
    "        name, img = image\n",
    "        image_img = Image.open(io.BytesIO(img))\n",
    "        \n",
    "        cat_id = name.split('/')[-2]\n",
    "        image_id = name.split('/')[-2] + '_' + name.split('/')[-1]\n",
    "        \n",
    "        print('image_id:',image_id)\n",
    "        \n",
    "        # Traitement de l'image\n",
    "        fill_color=(255, 255, 255, 0)\n",
    "        x, y = image_img.size\n",
    "        \n",
    "        if x < y:\n",
    "            size_x = y\n",
    "            size_y = y\n",
    "        else:\n",
    "            size_x = x\n",
    "            size_y = x\n",
    "        \n",
    "        new_im = Image.new('RGB', (size_x, size_y), fill_color)\n",
    "        new_im.paste(image_img, (int((size_x - x) / 2), int((size_y - y) / 2)))\n",
    "        img_cropped = new_im\n",
    "        \n",
    "        # Redimension de l'image\n",
    "        width = 100\n",
    "        img_rescaled = resizeimage.resize_cover(img_cropped, [width, width])\n",
    "        # Choix du format (original ou traité)\n",
    "        np_img = np.array(image_img)\n",
    "        #np_img = np.array(img_rescaled)\n",
    "        \n",
    "        # Calcul des descripteurs\n",
    "        if OPENCV == 'sift':\n",
    "            sift = cv2.SIFT_create()\n",
    "            keypoints_sift, desc = sift.detectAndCompute(np_img, None)\n",
    "        elif OPENCV == 'orb':\n",
    "            orb = cv2.ORB_create(nfeatures=100)\n",
    "            keypoints_orb, desc = orb.detectAndCompute(np_img, None)\n",
    "        \n",
    "        # On supprime les clés dont les descripteurs sont vides\n",
    "        # On construit un tableau que l'on remplit avec l'identifiant de l'image en cours de traitement\n",
    "        # Dans le cas où le traitement n'a produit aucun descripteur...\n",
    "        if desc is None:\n",
    "            # Dimensions du tableau: 1 ligne\n",
    "            ima = np.full(1, image_id)\n",
    "            ima_cat = np.full(1, cat_id)\n",
    "        # Si le traitement a produit au moins 1 descripteur...\n",
    "        else:\n",
    "            # Dimensions du tableau: autant de lignes que de descripteurs\n",
    "            ima = np.full(desc.shape[0], image_id)\n",
    "            ima_cat = np.full(desc.shape[0], cat_id)\n",
    "         \n",
    "    except:\n",
    "        ima = np.full(1, \"error\")\n",
    "        desc = None\n",
    "        ima_cat = np.full(1, \"error\")\n",
    "    \n",
    "    return ima_cat, ima, desc\n",
    "\n",
    "\n",
    "def get_images_descriptors(dataset_path):\n",
    "    \n",
    "    \"\"\" Cette fonction charge toutes les images et récupère les descripteurs via la fonction get_descriptors. \n",
    "   \n",
    "    - La fonction utilise la chaîne donnée en entrée (image_path) pour charger les images avec binaryFiles.\n",
    "    - Les données des images sont passées à la fonction get_descriptors qui renvoie les descripteurs par image.\n",
    "    - On obtient un RDD avec les catégories, images et descripteurs (on supprime les clés sans descripteur).\n",
    "    - La fonction renvoie la liste collectée des images et le RDD des descripteurs (non collecté).    \n",
    "    \"\"\"\n",
    "    \n",
    "    title(\"Calcul des descripteurs\",1)\n",
    "\n",
    "    title(\"Chargement des images (rdd_images)\",2)\n",
    "    \n",
    "    if LOCAL == True:\n",
    "        sdf_images = spark.read.format(\"binaryFile\") \\\n",
    "                               .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "                               .option(\"recursiveFileLookup\", \"true\") \\\n",
    "                               .load(dataset_path) \\\n",
    "                               .select(\"path\",\"content\")\n",
    "\n",
    "        rdd_images = sdf_images.rdd\n",
    "\n",
    "    elif LOCAL == False:\n",
    "        \n",
    "        rdd_images = sc.binaryFiles(image_path, minPartitions = MIN_PARTITION)\n",
    "        rdd_images = rdd_images.repartition(MIN_PARTITION)\n",
    " \n",
    "    print(rdd_images)\n",
    "    part_dim(rdd_images)\n",
    "        \n",
    "    if LOCAL and LOC_EXP:\n",
    "        clean(\"rdd/rdd_images.txt\")\n",
    "        rdd_images.saveAsTextFile(os.path.abspath(\"rdd/rdd_images.txt\"))\n",
    "\n",
    "    title(\"Catégories / Images / Descripteurs (rdd_cat_ima_desc)\",2) \n",
    "    # On utilise la fonction get_descriptors() pour extraire les descripteurs\n",
    "    # On récupères des tuples avec la catégorie, l'image et les descripteurs\n",
    "    rdd_cat_ima_desc = rdd_images.map(lambda img: get_descriptors(img))    \n",
    "    print(rdd_cat_ima_desc)\n",
    "    \n",
    "    # On supprime les clés où les descripteurs sont vides\n",
    "    # On mémorise le résultat (cache)\n",
    "    title(\"Catégories / Images / Descripteurs (rdd_cat_ima_desc_f)\",2)\n",
    "    \n",
    "    rdd_cat_ima_desc_f = rdd_cat_ima_desc.filter(lambda x: x[2] is not None).cache()\n",
    "    \n",
    "    print(rdd_cat_ima_desc_f)\n",
    "    \n",
    "    if LOCAL and LOC_EXP:\n",
    "        clean(\"rdd/rdd_cat_ima_desc_f.txt\")\n",
    "        rdd_cat_ima_desc_f.saveAsTextFile(os.path.abspath(\"rdd/rdd_cat_ima_desc_f.txt\"))\n",
    "    \n",
    "    # Mise à plat des catégories des images\n",
    "    title(\"Catégories (rdd_cat)\",2)\n",
    "    rdd_cat = rdd_cat_ima_desc_f.flatMap(lambda x: x[0])\n",
    "    print(rdd_cat)\n",
    "       \n",
    "    # Mise à plat des identifiants des images\n",
    "    title(\"Identifiants des images (rdd_ima)\",2)\n",
    "    rdd_ima = rdd_cat_ima_desc_f.flatMap(lambda x: x[1])\n",
    "    print(rdd_ima)\n",
    "    \n",
    "    # Mise à plat des descripteurs\n",
    "    title(\"Descripteurs (rdd_desc)\",2)\n",
    "    rdd_desc = rdd_cat_ima_desc_f.flatMap(lambda x: x[2])\n",
    "    print(rdd_desc)    \n",
    "    part_dim(rdd_desc)\n",
    "    \n",
    "    # On collecte les catégories des images\n",
    "    title(\"Collecte des catégories d\\'images (list_cat)\",2)\n",
    "    list_cat = rdd_cat.collect()\n",
    "    print(\"3 premières occurences:\",list_cat[0:3])\n",
    "    \n",
    "    # On collecte les identifiants des images\n",
    "    title(\"Collecte des identifiants des images (list_ima)\",2)\n",
    "    list_ima = rdd_ima.collect()\n",
    "    print(\"3 premières occurences:\",list_ima[0:3])\n",
    "        \n",
    "    # On construit un DataFrame Pandas avec les listes des catégories et identifiants des images\n",
    "    #title(\"Identifiants des images et des catégories (df_ima_cat)\",2)\n",
    "    df_ima_cat = pd.DataFrame({'ima': list_ima, 'cat': list_cat,})  \n",
    "    print('df_ima_cat:',df_ima_cat.shape)\n",
    "    df_ima_cat = df_ima_cat.drop_duplicates()\n",
    "    print('df_ima_cat (sans dup):',df_ima_cat.shape)\n",
    "    #print('3 premières occurences:\\n\\n',df_ima_cat.head(3))\n",
    "    \n",
    "    # On exporte le DataFrame Pandas\n",
    "    if LOCAL:\n",
    "        df_ima_cat.to_csv('temp/df_ima_cat.csv', index=False)\n",
    "        df_ima_cat = pd.read_csv('temp/df_ima_cat.csv',encoding='utf-8',low_memory=False)\n",
    "    else:\n",
    "        csv_buffer = StringIO()\n",
    "        df_ima_cat.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        # upload\n",
    "        s3_resource.Object('oc-ds-p8', 'temp/df_ima_cat.csv').put(Body=csv_buffer.getvalue(), ACL='public-read')\n",
    "        # download\n",
    "        s3_resource.Object('oc-ds-p8', 'temp/df_ima_cat.csv').download_file(f'/tmp/df_ima_cat.csv')\n",
    "        df_ima_cat = pd.read_csv('/tmp/df_ima_cat.csv',encoding='utf-8',low_memory=False)  \n",
    "        df_ima_cat = df_ima_cat[['ima','cat']]\n",
    "   \n",
    "    sdf_ima_cat = spark.createDataFrame(df_ima_cat)\n",
    "    title(\"Identifiants des images et des catégories (sdf_ima_cat)\",2)\n",
    "    sdf_ima_cat.printSchema()\n",
    "    sdf_ima_cat.limit(3).show()\n",
    "    \n",
    "    return rdd_desc, list_ima, sdf_ima_cat, df_ima_cat\n",
    "\n",
    "\n",
    "def kmeans_train(rdd_desc, nb_cat):\n",
    "    \"\"\" Classification non supervisée des descripteurs avec l'algorithme K-Means. \"\"\"\n",
    "    \n",
    "    title(\"Classification non supervisée des descripteurs avec K-Means\",1)\n",
    "    \n",
    "    title(\"Modèle K-Means (km_model)\",2)\n",
    "    # Détermination du nombre de clusters\n",
    "    nb_clusters = nb_cat * 10\n",
    "    # Entraînement du modèle    \n",
    "    km_model = KMeans.train(rdd_desc, nb_clusters, maxIterations=500, initializationMode=\"random\")    \n",
    "    print(km_model)\n",
    "    print('\\nNombre de clusters:',nb_clusters,'\\n')\n",
    "    \n",
    "    return nb_clusters, km_model\n",
    "\n",
    "\n",
    "def kmeans_pred(km_model, rdd_desc):\n",
    "    \"\"\" Prédictions des descripteurs avec K-Means.\"\"\"\n",
    "    \n",
    "    title(\"Prédictions des descripteurs avec K-Means\",1)    \n",
    "    \n",
    "    title(\"Prédictions (rdd_km_pred)\",2)\n",
    "    # Prédiction    \n",
    "    rdd_km_pred = km_model.predict(rdd_desc)    \n",
    "    print(rdd_km_pred)    \n",
    "    part_dim(rdd_km_pred)   \n",
    "    title(\"Collecte des prédictions (list_km_pred)\",2)\n",
    "    # Collecte des prédictions\n",
    "    list_km_pred = rdd_km_pred.collect()\n",
    "    print(list_km_pred[0:10],'\\n')\n",
    "    \n",
    "    return list_km_pred\n",
    "\n",
    "\n",
    "def bow_creation(list_ima, list_km_pred):\n",
    "    \"\"\" Création d'un bag of words pour les images.\n",
    "    \n",
    "    - Encodage des identifiants d'image dans la variable 'id'.\n",
    "    - Parallélisation les données dans un DataFrame spark converti en RDD.\n",
    "    - On applique un map et reduceByKey pour obtenir la liste des clusters pour chaque image.\n",
    "    - Historisation: on compte le nombre de clusters par image (CountVectorizer).\n",
    "    \"\"\"\n",
    "    \n",
    "    title(\"Création du bag of words\",1)\n",
    "    \n",
    "    title(\"Encodage des identifiants d'images et concatenation avec les prédictions (clusters K-Means)\",2)\n",
    "    \n",
    "    title(\"Encodage des identifiants d'images (sdf_ima_label)\",3)\n",
    "    \n",
    "    # Encodage des identifiants des images\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    label_ima = le.fit_transform(list_ima)\n",
    "    \n",
    "    df_ima_label = pd.DataFrame()\n",
    "    df_ima_label['IMA'] = list_ima\n",
    "    df_ima_label['image_id'] = label_ima\n",
    "    df_ima_label['prediction'] = list_km_pred\n",
    "    \n",
    "    # On exporte le DataFrame Pandas\n",
    "    if LOCAL:\n",
    "        df_ima_label.to_csv(os.path.abspath(\"temp/df_ima_label.csv\"), index=False)\n",
    "        df_ima_label = pd.read_csv('temp/df_ima_label.csv',encoding='utf-8',low_memory=False)\n",
    "    else:\n",
    "        csv_buffer = StringIO()\n",
    "        df_ima_label.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')        \n",
    "        # upload\n",
    "        s3_resource.Object('oc-ds-p8', \"temp/df_ima_label.csv\") \\\n",
    "                   .put(Body=csv_buffer.getvalue(), ACL='public-read')     \n",
    "        # download\n",
    "        s3_resource.Object('oc-ds-p8', 'temp/df_ima_label.csv').download_file(f'/tmp/df_ima_label.csv')\n",
    "        df_ima_label = pd.read_csv('/tmp/df_ima_label.csv',encoding='utf-8',low_memory=False)\n",
    "    \n",
    "    sdf_ima_label = spark.createDataFrame(df_ima_label)\n",
    "    sdf_ima_label.limit(10).show()\n",
    "    \n",
    "    # Concatenation des identifiants des images (numériques) et des prédictions (clusters K-Means)\n",
    "    numpy_arr = np.concatenate((np.array(label_ima).reshape(-1,1), \n",
    "                                np.array(list_km_pred).reshape(-1,1)), axis=1)\n",
    "    \n",
    "    # On construit un DataFrame Pandas avec comme variables les identifiants des images et les prédictions\n",
    "    # L'identifiant unique du DataFrame représente les descripteurs (1 image a plusieurs descripteurs)\n",
    "    df_ima_pred = pd.DataFrame(numpy_arr, columns=['id', 'prediction'])\n",
    "    \n",
    "    title(\"Prédictions (clusters K-Means) par image (sdf_ima_pred)\",3)\n",
    "    # On convertit le DataFrame Pandas en DataFrame Spark\n",
    "    sdf_ima_pred = spark.createDataFrame(df_ima_pred)\n",
    "    sdf_ima_pred.printSchema()\n",
    "    sdf_ima_pred.limit(10).show()\n",
    "    \n",
    "    title(\"Liste des clusters par image (Map + reduceByKey)\",2)\n",
    "    \n",
    "    title(\"Clusters par image (rdd_words)\",3)\n",
    "    # On liste les clusters pour chaque image  \n",
    "    rdd_ima_pred = sdf_ima_pred.rdd.map(lambda x:x)\n",
    "    rdd_words = rdd_ima_pred.reduceByKey(lambda a,b: str(a) + ',' + str(b)) \n",
    "    print(rdd_words)    \n",
    "    part_dim(rdd_words)\n",
    "    \n",
    "    if LOCAL and LOC_EXP:\n",
    "        clean(\"rdd/rdd_words.txt\")\n",
    "        rdd_words.saveAsTextFile(os.path.abspath(\"rdd/rdd_words.txt\"))\n",
    "    \n",
    "    title(\"\\nListe de 'words' par image (sdf_worcds)\",3) \n",
    "    # On construit une liste de 'words' à partir de la liste des clusters (string)    \n",
    "    sdf_words = rdd_words.map(lambda tupl_words: (tupl_words[0], str(tupl_words[1]).split(','))) \\\n",
    "                         .toDF(['image_id','words'])\n",
    "    \n",
    "    sdf_words.printSchema()\n",
    "    sdf_words.limit(10).show()\n",
    "    \n",
    "    title(\"Création du bag of words à partir des listes de 'words' associées aux images (CountVectorizer)\",2)\n",
    "\n",
    "    title(\"Bag of words (sdf_bow)\",3)\n",
    "    # Création d'un vecteur de 'words' par image    \n",
    "    vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"bag_of_words\")    \n",
    "    vectorizer_transformer = vectorizer.fit(sdf_words)\n",
    "    sdf_bow = vectorizer_transformer.transform(sdf_words).select('image_id', 'bag_of_words')\n",
    "    sdf_bow.printSchema()\n",
    "    sdf_bow.limit(10).show()\n",
    "         \n",
    "    return sdf_bow, sdf_ima_label, df_ima_label\n",
    "\n",
    "\n",
    "def bow_save(sdf_bow, list_ima, name):\n",
    "    \"\"\" Sauvegarde du bag of words en local ou sur S3. \"\"\"\n",
    "    \n",
    "    title(\"Sauvegarde du bag of words\",1)\n",
    "\n",
    "    # On transforme le bag of words (DataFrame Spark) en DataFrame Pandas\n",
    "    df_bow = sdf_bow.toPandas()\n",
    "    \n",
    "    title(\"Bag of words (df_bow)\",2)\n",
    "    print(df_bow.head(),'\\n')\n",
    "    \n",
    "    # On exporte le DataFrame Pandas\n",
    "    if LOCAL:\n",
    "        df_bow.to_csv('out/'+name, index=False)\n",
    "    else:\n",
    "        csv_buffer = StringIO()\n",
    "        df_bow.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.Object('oc-ds-p8', 'out/'+name).put(Body=csv_buffer.getvalue(), ACL='public-read')\n",
    "        \n",
    "    return df_bow\n",
    "\n",
    "\n",
    "def bow_transform(df_bow, nb_clusters, name, list_ima, df_ima_cat, df_ima_label):\n",
    "    \"\"\" Transformation du bag of words pour affichage en mode tableau. \"\"\"\n",
    "    \n",
    "    if LOCAL:\n",
    "        bag_of_words = pd.read_csv('out/df_bow_'+name,encoding='utf-8',low_memory=False)\n",
    "    else:\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.Object('oc-ds-p8', 'out/df_bow_'+name).download_file(f'/tmp/df_bow_'+name)\n",
    "        bag_of_words = pd.read_csv('/tmp/df_bow_'+name,encoding='utf-8',low_memory=False)\n",
    "\n",
    "    list_image_id = bag_of_words['image_id']\n",
    "    \n",
    "    x0 = bag_of_words['bag_of_words']\n",
    "    # eval: transformation string en tuple\n",
    "    x01 = x0.transform(lambda x: eval(x.replace(str(nb_clusters)+',','').replace('(','').replace(')','')))\n",
    "    x01df = pd.DataFrame(x01)\n",
    "\n",
    "    visual_words = list(range(nb_clusters))\n",
    "    list_vw_val_all = []\n",
    "    \n",
    "    # Pour chaque image\n",
    "    for i in list(range(x01.shape[0])):\n",
    "        x01 = x01df.iloc[i,0]\n",
    "\n",
    "        list_vw_val = []\n",
    "        ind = 0\n",
    "\n",
    "        # Pour chaque visual word\n",
    "        for vw in visual_words:\n",
    "            # Si le visual word existe dans le tuple de l'image\n",
    "            if vw in x01[0]:\n",
    "                # On stocke sa valeur dans la liste des valeurs de visual words\n",
    "                list_vw_val.append(x01[1][ind])\n",
    "                # On incrémente l'indice de localisation des valeurs\n",
    "                ind = ind + 1\n",
    "            # Si le visual word n'est pas présent pour l'image\n",
    "            else:\n",
    "                # On ajoute 0 à la liste concernant ce visual word\n",
    "                list_vw_val.append(0)\n",
    "\n",
    "        list_vw_val_all.append(list_vw_val)\n",
    "\n",
    "    bow = pd.DataFrame(list_vw_val_all)\n",
    "    bow[\"image_id\"] = list_image_id\n",
    "    \n",
    "    del df_ima_label['prediction']\n",
    "    df_ima_label = df_ima_label.drop_duplicates()\n",
    "    \n",
    "    bow = bow.merge(df_ima_label, left_on = 'image_id', right_on = 'image_id')\n",
    "\n",
    "    list1 = ['image_id','IMA']\n",
    "    list2 = bow.columns.tolist()[:-2]\n",
    "    list1.extend(list2)\n",
    "    bow = bow[list1]\n",
    "\n",
    "    bow = df_ima_cat.merge(bow, left_on='ima', right_on='IMA')\n",
    "    del bow['image_id']\n",
    "    del bow['IMA']\n",
    "        \n",
    "    title(\"Bag of words\",2)\n",
    "    print(bow.iloc[0:5,0:8],'\\n')\n",
    "    print('Dimensions du jeu de données:',bow.shape,'\\n')\n",
    "    \n",
    "    # On exporte le DataFrame Pandas\n",
    "    if LOCAL:\n",
    "        bow.to_csv('out/bow_'+name, index=False)\n",
    "    else:\n",
    "        csv_buffer = StringIO()\n",
    "        bow.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.Object('oc-ds-p8', 'out/bow_'+name).put(Body=csv_buffer.getvalue(), ACL='public-read')\n",
    "\n",
    "\n",
    "def bow_reduce(sdf_bow, nb_clusters, sdf_ima_cat, data_set_name, sdf_ima_label):\n",
    "    \"\"\" Réduction des dimensions du bag of words avec la méthode ACP. \"\"\"\n",
    "    \n",
    "    title('Réduction de dimension PCA',1)\n",
    "    \n",
    "    pca_dim = int(nb_clusters-(nb_clusters*0.3))\n",
    "    pca = PCA(k=pca_dim, inputCol=\"bag_of_words\", outputCol=\"features\")\n",
    "    model = pca.fit(sdf_bow)\n",
    "    sdf_features = model.transform(sdf_bow).select(\"features\")\n",
    "    \n",
    "    title(\"Résultats de la PCA (sdf_features)\",2)\n",
    "    sdf_features.printSchema()\n",
    "    #sdf_features.show(truncate=False)\n",
    "    sdf_features.limit(10).show()\n",
    "    \n",
    "    # Fonction de création d'un index dans un DataFrame Spark    \n",
    "    def with_column_index(sdf):\n",
    "        new_schema = StructType(sdf.schema.fields + [StructField(\"ColumnIndex\", LongType(), False),])\n",
    "        return sdf.rdd.zipWithIndex().map(lambda row: row[0] + (row[1],)).toDF(schema=new_schema)\n",
    "\n",
    "    df1_ci = with_column_index(sdf_bow)\n",
    "    df2_ci = with_column_index(sdf_features)\n",
    "    sdf_ima_features = df1_ci.join(df2_ci, df1_ci.ColumnIndex == df2_ci.ColumnIndex, 'inner').drop(\"ColumnIndex\")\n",
    "    title(\"Jointure entre les ids des images et les features (sdf_ima_features)\",2)\n",
    "    sdf_ima_features.printSchema()\n",
    "    sdf_ima_features.limit(10).show()    \n",
    "\n",
    "    sdf_ima_label_2 = sdf_ima_label.drop('prediction')  \n",
    "    sdf_ima_lab_features = sdf_ima_label_2.join(sdf_ima_features, \"image_id\")  \n",
    "    \n",
    "    sdf_ima_cat_features = sdf_ima_lab_features.join(sdf_ima_cat, \"IMA\")    \n",
    "    sdf_cat_features = sdf_ima_cat_features.drop('IMA')\n",
    "    sdf_cat_features = sdf_ima_cat_features.drop('image_id')\n",
    "    sdf_cat_features = sdf_ima_cat_features.drop('bag_of_words')    \n",
    "    \n",
    "    sdf_cat_features = sdf_cat_features.distinct()\n",
    "    \n",
    "    title(\"Jointure entre les catégories et les features (sdf_cat_features)\",2)\n",
    "    sdf_cat_features.printSchema()\n",
    "    sdf_cat_features.limit(10).show()\n",
    "\n",
    "    # Encodage de la variable catégorie\n",
    "    indexer = StringIndexer(inputCol=\"cat\", outputCol=\"label\").fit(sdf_cat_features)\n",
    "    sdf_lab_features = indexer.transform(sdf_cat_features)\n",
    "    sdf_lab_features = sdf_lab_features.drop(\"cat\")    \n",
    "    sdf_lab_features = sdf_lab_features.select(\"label\",\"features\")\n",
    "    \n",
    "    title(\"Encodage de la variable catégories (sdf_lab_features)\",2)\n",
    "    sdf_lab_features.printSchema()\n",
    "    sdf_lab_features.limit(10).show()\n",
    "    \n",
    "    df_lab_features = sdf_lab_features.toPandas()\n",
    "    df_lab_features = df_lab_features.drop_duplicates()\n",
    "\n",
    "    # On exporte le DataFrame Pandas\n",
    "    if LOCAL:\n",
    "        df_lab_features.to_csv(os.path.abspath(\"temp/df_lab_features_0.csv\"), index=False)\n",
    "        df_lab_features = pd.read_csv('temp/df_lab_features_0.csv',encoding='utf-8',low_memory=False)\n",
    "    else:\n",
    "        csv_buffer = StringIO()\n",
    "        df_lab_features.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')        \n",
    "        # upload\n",
    "        s3_resource.Object('oc-ds-p8', \"temp/df_lab_features_0.csv\") \\\n",
    "                   .put(Body=csv_buffer.getvalue(), ACL='public-read')      \n",
    "        # download\n",
    "        s3_resource.Object('oc-ds-p8', 'temp/df_lab_features_0.csv').download_file(f'/tmp/df_lab_features_0.csv')\n",
    "        df_lab_features = pd.read_csv('/tmp/df_lab_features_0.csv',encoding='utf-8',low_memory=False)\n",
    "    \n",
    "    df_lab_features = df_lab_features[['label','features']]\n",
    "\n",
    "    feat_list_all = []    \n",
    "    for i in list(range(df_lab_features.shape[0])):\n",
    "        for feat_str in df_lab_features.iloc[i,1:]:\n",
    "            feat_list = ast.literal_eval(feat_str)\n",
    "            feat_list_all.append(feat_list)\n",
    "\n",
    "    df_lab_features_feat = pd.DataFrame(feat_list_all)     \n",
    "    df_lab_features_lab = df_lab_features[['label']]\n",
    "\n",
    "    df_lab_features = df_lab_features_lab.merge(df_lab_features_feat, left_index=True, right_index=True)\n",
    "\n",
    "    # On exporte le DataFrame Pandas\n",
    "    if LOCAL:\n",
    "        df_lab_features.to_csv(os.path.abspath(\"out/df_lab_features_\" + data_set_name + \".csv\"), index=False)\n",
    "    else:\n",
    "        csv_buffer = StringIO()\n",
    "        df_lab_features.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.Object('oc-ds-p8', \"out/df_lab_features_\" + data_set_name + \".csv\") \\\n",
    "        .put(Body=csv_buffer.getvalue(), ACL='public-read')\n",
    "\n",
    "    title(\"Bag of words après réduction de dimension (df_lab_features)\",2)\n",
    "    print(df_lab_features.iloc[0:9,0:7])\n",
    "    print('\\nDimensions du nouveau jeu de données avec les étiquettes (df_lab_features):',df_lab_features.shape)\n",
    "        \n",
    "    if OPT == True:\n",
    "        \n",
    "        # df_features\n",
    "        df_features = sdf_features.toPandas()\n",
    "            \n",
    "        # On exporte le DataFrame Pandas\n",
    "        if LOCAL:\n",
    "            df_features.to_csv(os.path.abspath(\"temp/df_features.csv\"), index=False)\n",
    "            df_features = pd.read_csv('temp/df_features.csv',encoding='utf-8',low_memory=False)\n",
    "        else:\n",
    "            csv_buffer = StringIO()\n",
    "            df_features.to_csv(csv_buffer)\n",
    "            s3_resource = boto3.resource('s3')\n",
    "            # upload\n",
    "            s3_resource.Object('oc-ds-p8', \"temp/df_features.csv\").put(Body=csv_buffer.getvalue(), ACL='public-read')\n",
    "            # download\n",
    "            s3_resource.Object('oc-ds-p8', 'temp/df_features.csv').download_file(f'/tmp/df_features.csv')\n",
    "            df_features = pd.read_csv('/tmp/df_features.csv',encoding='utf-8',low_memory=False)\n",
    "        \n",
    "        df_features = df_features[['features']]\n",
    "        \n",
    "        feat_list_all = []    \n",
    "        for i in list(range(df_features.shape[0])):\n",
    "            for feat_str in df_features.iloc[i,:]:\n",
    "                feat_list = ast.literal_eval(feat_str)\n",
    "                feat_list_all.append(feat_list)\n",
    "\n",
    "        title(\"\\nBag of words après réduction de dimension (df_features)\",2)\n",
    "        df_features = pd.DataFrame(feat_list_all)\n",
    "        print(df_features.iloc[0:5,0:5])\n",
    "        print('\\nDimension du nouveau jeu de données (df_features):',df_features.shape,'\\n')\n",
    "    \n",
    "    return sdf_lab_features, sdf_features, pca_dim\n",
    "\n",
    "\n",
    "def rf_classification(sdf_lab_features):\n",
    "    \n",
    "    title('Classification RF',1)\n",
    "    \n",
    "    rf = RandomForestClassifier(numTrees=1000, maxDepth=10, labelCol=\"label\", seed=42)\n",
    "    # Valeur max pour maxDepth = 30\n",
    "    rf_model = rf.fit(sdf_lab_features)\n",
    "    \n",
    "    return rf_model\n",
    " \n",
    "\n",
    "def rf_prediction(sdf_features, sdf_lab_features, rf_model):\n",
    "    \n",
    "    title('Prédictions RF',1)\n",
    "    \n",
    "    test_lab_pred = rf_model.transform(sdf_features).select(\"features\", \"prediction\")\n",
    "    title('Prédictions RF (test_lab_pred)',2)\n",
    "    print(type(test_lab_pred))\n",
    "    print(test_lab_pred)\n",
    "    \n",
    "    test_lab_pred_2 = rf_model.transform(sdf_lab_features)\n",
    "    predictionAndLabels = test_lab_pred_2.select(\"prediction\", \"label\")\n",
    "    title('Prédictions (predictionAndLabels)',2)\n",
    "    print(type(predictionAndLabels))\n",
    "    print(predictionAndLabels)\n",
    "    predictionAndLabels.limit(10).show()\n",
    "    \n",
    "    predictionAndLabels_2 = test_lab_pred_2.select(\"prediction\", \"label\", \"features\")\n",
    "    predictionAndLabels_2.limit(10).show()\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    accuracy = round(evaluator.evaluate(predictionAndLabels),2)\n",
    "    print(\"Test set accuracy (RF) = \" + str(accuracy))\n",
    "    \n",
    "    return test_lab_pred\n",
    "\n",
    "  \n",
    "def mlp_classification(sdf_lab_features, pca_dim, nb_cat):\n",
    "    \n",
    "    title('Classification MLP',1)\n",
    "    \n",
    "    layers = [pca_dim, pca_dim, pca_dim, nb_cat]\n",
    "    \n",
    "    mlp = MultilayerPerceptronClassifier(layers=layers, seed=rs_)\n",
    "    mlp.setMaxIter(200)\n",
    "    mlp.setBlockSize(128)\n",
    "    mlp_model = mlp.fit(sdf_lab_features)\n",
    "    mlp_model.setFeaturesCol(\"features\")    \n",
    "    \n",
    "    print(\"Modèle:\\n\\n\",mlp_model,'\\n')\n",
    "    \n",
    "    return mlp_model\n",
    "\n",
    "\n",
    "def mlp_prediction(sdf_features, sdf_lab_features, mlp_model):\n",
    "    \n",
    "    title('Prédictions MLP',1)\n",
    "    \n",
    "    if OPT == True:\n",
    "        test_pred = mlp_model.predict(sdf_features.head().features)\n",
    "        test_pred_raw = mlp_model.predictRaw(sdf_features.head().features)\n",
    "        test_pred_prob = mlp_model.predictProbability(sdf_features.head().features)\n",
    "\n",
    "    test_lab_pred = mlp_model.transform(sdf_features).select(\"features\", \"prediction\")\n",
    "    \n",
    "    title('Prédictions (test_lab_pred)',2)\n",
    "    print(test_lab_pred)\n",
    "    test_lab_pred.limit(10).show()\n",
    "    \n",
    "    test_lab_pred_2 = mlp_model.transform(sdf_lab_features)\n",
    "    predictionAndLabels = test_lab_pred_2.select(\"prediction\", \"label\")  \n",
    "    \n",
    "    predictionAndLabels_2 = test_lab_pred_2.select(\"prediction\", \"label\", \"features\")\n",
    "    title('Prédictions (predictionAndLabels_2)',2)\n",
    "    print(predictionAndLabels_2)\n",
    "    predictionAndLabels_2.limit(10).show()\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    accuracy = round(evaluator.evaluate(predictionAndLabels),2)\n",
    "    print(\"Test set accuracy (MLP) = \" + str(accuracy))\n",
    "    \n",
    "    return test_lab_pred\n",
    "    \n",
    "    \n",
    "def cls_eval(sdf_ima_cat, test_lab_pred, sdf_bow, sdf_ima_label):\n",
    "    \n",
    "    title('Evaluation',1)\n",
    "\n",
    "    # Fonction de création d'un index dans un DataFrame Spark    \n",
    "    def with_column_index(sdf):\n",
    "        new_schema = StructType(sdf.schema.fields + [StructField(\"ColumnIndex\", LongType(), False),])\n",
    "        return sdf.rdd.zipWithIndex().map(lambda row: row[0] + (row[1],)).toDF(schema=new_schema)\n",
    "\n",
    "    df1_ci = with_column_index(sdf_bow)\n",
    "    df2_ci = with_column_index(test_lab_pred)\n",
    "    sdf_ima_features = df1_ci.join(df2_ci, df1_ci.ColumnIndex == df2_ci.ColumnIndex, 'inner').drop(\"ColumnIndex\")\n",
    "    \n",
    "    title('Jointure entre les identifiants des images et les features (sdf_ima_features)',2)\n",
    "    sdf_ima_features.printSchema()\n",
    "    sdf_ima_features.limit(10).show()    \n",
    "\n",
    "    sdf_ima_label_2 = sdf_ima_label.drop('prediction') \n",
    "    sdf_ima_lab_features = sdf_ima_label_2.join(sdf_ima_features, \"image_id\")  \n",
    "    \n",
    "    sdf_ima_cat_features = sdf_ima_lab_features.join(sdf_ima_cat, \"IMA\")    \n",
    "    sdf_cat_features = sdf_ima_cat_features.drop('IMA')\n",
    "    sdf_cat_features = sdf_ima_cat_features.drop('image_id')\n",
    "    sdf_cat_features = sdf_ima_cat_features.drop('bag_of_words')    \n",
    "    \n",
    "    sdf_cat_features = sdf_cat_features.distinct()\n",
    "    \n",
    "    title(\"Jointure entre les catégories et les features (sdf_cat_features)\",2)\n",
    "    sdf_cat_features.printSchema()\n",
    "    sdf_cat_features.limit(10).show()\n",
    "\n",
    "    # Encodage de la variable catégorie\n",
    "    indexer = StringIndexer(inputCol=\"cat\", outputCol=\"label\").fit(sdf_cat_features)\n",
    "    sdf_lab_features = indexer.transform(sdf_cat_features)  \n",
    "    sdf_lab_features = sdf_lab_features.select(\"ima\",\"cat\",\"label\",\"prediction\",\"features\")\n",
    "    \n",
    "    title('Encodage de la variable catégories (sdf_lab_features)',2)\n",
    "    sdf_lab_features.printSchema()\n",
    "    sdf_lab_features.limit(10).show()\n",
    "    \n",
    "    # DataFrame Spark --> DataFrame Pandas\n",
    "    df_lab_features = sdf_lab_features.toPandas() \n",
    "    df_lab_features['occur'] = pd.Series(1, range(df_lab_features.shape[0])) # On ajoute une variable contenant\n",
    "                                                                             # uniquement la valeur 1\n",
    "                                                           # afin de pouvoir réaliser une transformation pivot\n",
    "\n",
    "    df_mlp_2 = pd.pivot_table(df_lab_features, values='occur',index=['label'], \\\n",
    "                              columns=['prediction'], aggfunc=np.sum)\n",
    "\n",
    "    df_mlp_2.fillna(0, inplace=True) # On remplace les valeurs nulles par 0\n",
    "    df_mlp_2 = round(100*(df_mlp_2.T / df_mlp_2.T.sum()).T) # On calcule les % de prédictions\n",
    "    \n",
    "    title('Catégories réelles (label) vs Prédictions (prediction)',2)\n",
    "    \n",
    "    print(df_mlp_2)\n",
    "\n",
    "\n",
    "    \n",
    "##############################\n",
    "### LANCEMENT DU PROGRAMME ###\n",
    "##############################\n",
    "\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "title(\"> > > > TRAITEMENT DU JEU DE DONNES D'ENTRAINEMENT < < < <\",1)  \n",
    "t = time.time()\n",
    "\n",
    "# Récupération des images\n",
    "nb_cat, image_path, dataset_path = get_path('Training/')\n",
    "time_calc('Récupération des images','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "# Extraction des descripteurs des images\n",
    "rdd_desc, list_ima, sdf_ima_cat, df_ima_cat = get_images_descriptors(dataset_path)\n",
    "time_calc('Extraction des descripteurs des images','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "# Clustering K-Means\n",
    "nb_clusters, km_model = kmeans_train(rdd_desc, nb_cat)\n",
    "time_calc('Clustering K-Means','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "list_km_pred = kmeans_pred(km_model, rdd_desc)\n",
    "time_calc('Prédiction K-Means','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "# Création du bag of words\n",
    "sdf_bow, sdf_ima_label, df_ima_label = bow_creation(list_ima, list_km_pred)\n",
    "df_bow = bow_save(sdf_bow, list_ima, 'df_bow_train.csv')\n",
    "bow_transform(df_bow, nb_clusters, 'train.csv', list_ima, df_ima_cat, df_ima_label)  \n",
    "time_calc('Création du bag of words','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "# Réduction de dimension\n",
    "sdf_lab_features ,sdf_features, pca_dim = bow_reduce(sdf_bow, nb_clusters,\n",
    "                                                     sdf_ima_cat, \"train\", sdf_ima_label)   \n",
    "print('')\n",
    "time_calc('Réduction de dimension','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "if CLS:\n",
    "    \n",
    "    if LOCAL:\n",
    "        # Classification MLP\n",
    "        mlp_model = mlp_classification(sdf_lab_features, pca_dim, nb_cat)\n",
    "    else:\n",
    "        # Classification RF\n",
    "        rf_model = rf_classification(sdf_lab_features)\n",
    "\n",
    "    time_calc('Classification','',0,0)\n",
    "    t = time.time()\n",
    "    print('')\n",
    "\n",
    "\n",
    "title(\"> > > > TRAITEMENT DU JEU DE DONNEES DE TEST < < < <\",1)\n",
    "t = time.time()\n",
    "\n",
    "# Récupération des images\n",
    "nb_cat, image_path, dataset_path = get_path('Test/')\n",
    "time_calc('Récupération des images - Test','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "# Extraction des descripteurs des images\n",
    "rdd_desc, list_ima, sdf_ima_cat, df_ima_cat = get_images_descriptors(dataset_path)\n",
    "time_calc('Extraction des descripteurs des images - Test','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "#nb_clusters, km_model = kmeans_train(rdd_desc, nb_cat)\n",
    "#time_calc('Test - Clustering K-Means','',1,0)\n",
    "#t = time.time()\n",
    "#print('')\n",
    "# Prédictions K-Means\n",
    "list_km_pred = kmeans_pred(km_model, rdd_desc)\n",
    "time_calc('Prédiction K-Means - Test','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "# Création du bag of words\n",
    "sdf_bow, sdf_ima_label, df_ima_label = bow_creation(list_ima, list_km_pred)\n",
    "df_bow = bow_save(sdf_bow, list_ima, 'df_bow_test.csv') \n",
    "bow_transform(df_bow, nb_clusters, 'test.csv', list_ima, df_ima_cat, df_ima_label)\n",
    "\n",
    "time_calc('Création du bag of words - Test','',0,0)\n",
    "t = time.time()\n",
    "\n",
    "# Réduction de dimension\n",
    "sdf_lab_features, sdf_features, pca_dim = bow_reduce(sdf_bow, nb_clusters,\n",
    "                                                     sdf_ima_cat, \"test\", sdf_ima_label)    \n",
    "print('')\n",
    "time_calc('Réduction de dimension - Test','',0,0)\n",
    "t = time.time()\n",
    "print('')\n",
    "\n",
    "if CLS:\n",
    "    \n",
    "    if LOCAL:\n",
    "        # Prédictions MLP\n",
    "        test_lab_pred = mlp_prediction(sdf_features, sdf_lab_features, mlp_model)\n",
    "    else:\n",
    "        # Prédictions RF\n",
    "        test_lab_pred = rf_prediction(sdf_features, sdf_lab_features, rf_model)\n",
    "    \n",
    "    print('')\n",
    "    time_calc('Prédiction - Test','',0,0)\n",
    "    t = time.time()\n",
    "    print('')\n",
    "    \n",
    "    # Evaluation\n",
    "    cls_eval(sdf_ima_cat, test_lab_pred, sdf_bow, sdf_ima_label)\n",
    "    print('')\n",
    "    time_calc('Evaluation - Test','',0,0)\n",
    "    t = time.time()\n",
    "\n",
    "print('')    \n",
    "time_calc('Fin des traitements','',1,1)\n",
    "\n",
    "title(\"> > > > Traitements finalisés < < < <\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
